{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27cb0d22-f68c-4d89-9426-0cb0ef5ad86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,re, os, glob, json, string, pprint\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from snowballstemmer import TurkishStemmer \n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da360665-ec6b-4c02-910d-fef63c8028d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b5d09b-1eeb-4c9d-b8e4-c75ce6dd9296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"C://Users/onkol/Desktop/2021-01-20220322T055600Z-001/2021-01/*.json\" \n",
    "files = glob.glob(path)\n",
    "\n",
    "ictihats = []\n",
    "suc_array = []\n",
    "\n",
    "        \n",
    "top_ten_crimes = []\n",
    "with open(\"data.json\", encoding='utf-8') as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    top_ten_crimes = list(map(lambda x: x[0], data[:10]))\n",
    "    \n",
    "\n",
    "for file in files:\n",
    "    f = open(file, \"r\", encoding='utf-8')\n",
    "    jsonData = json.loads(f.read())\n",
    "    key = jsonData[\"Suç\"] if jsonData[\"Suç\"] != '' else \"other\"\n",
    "    if key in top_ten_crimes:\n",
    "        ictihats.append(jsonData['ictihat'].strip())\n",
    "        suc = jsonData[\"Suç\"] if jsonData[\"Suç\"] != '' else \"other\"\n",
    "        suc_array.append(suc.strip())\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59b8de-799b-4a9e-9843-d37938bf183d",
   "metadata": {},
   "source": [
    "## Create labels from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8988295e-bd09-43d4-ac70-ce241b775dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def has_key(dict, key): \n",
    "#     if key in dict.keys(): \n",
    "#         return 1 \n",
    "#     return 0\n",
    "\n",
    "# labels = {} \n",
    "# def readLabels(fileName): \n",
    "#     f = open(file, \"r\", encoding='utf-8') \n",
    "#     jsonData = json.loads(f.read())\n",
    "#     key = jsonData[\"Suç\"] if jsonData[\"Suç\"] != '' else \"other\"\n",
    "#     if (not has_key(labels, key)):\n",
    "#         labels[key] = 1    \n",
    "#     else :\n",
    "#         labels[key] = labels[key] + 1\n",
    "\n",
    "# path = \"C://Users/onkol/Desktop/2021-01-20220322T055600Z-001/2021-01/*.json\" \n",
    "# files = glob.glob(path)\n",
    "\n",
    "# for file in files: \n",
    "#     readLabels(file)\n",
    "\n",
    "# sort_orders = sorted(labels.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# jsonString = json.dumps(sort_orders, indent = 4, ensure_ascii=False)\n",
    "# jsonFile = open(\"data.json\", \"w\", encoding='utf-8')\n",
    "# jsonFile.write(jsonString)\n",
    "# jsonFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f9426-0989-4e9c-86ca-9761ae0c9fe5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72cb2224-5f34-4e3b-a69c-e7dd2d206800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ictihats_lower = []\n",
    "for i in ictihats:\n",
    "    i = i.lower()\n",
    "    ictihats_lower.append(i)\n",
    "    \n",
    "\n",
    "ictihats_punctuation = []\n",
    "\n",
    "file = open(\"stopwords.txt\", \"r\", newline='', encoding='utf-8')\n",
    "result = file.read()\n",
    "stopwords = word_tokenize(result)\n",
    "word_size = 20\n",
    "\n",
    "for ictihat in ictihats_lower:\n",
    "    ictihat = ictihat.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    #print(ictihat)\n",
    "    tokenized_words = ([word for word in ictihat.split() if word not in stopwords and len(word) > 1 and re.match(\"^[A-Z0-9a-zğüşöçİĞÜŞÖÇ]*$\", word)])\n",
    "    #print(tokenized_words)\n",
    "    ictihats_punctuation.append(' '.join(tokenized_words[:word_size]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# print(ictihats_punctuation[:10])   \n",
    "# preprocessed_ictihats = []\n",
    "# for i in ictihats_punctuation:\n",
    "#     preprocessed_ictihats.append(list(filter(lambda x: x != \"\",i.split(\" \"))))\n",
    "\n",
    "# print(preprocessed_ictihats)\n",
    "turkStem = TurkishStemmer()\n",
    "for index, ictihat in enumerate(ictihats_punctuation,start=0):\n",
    "    #print([turkStem.stemWord(word) for word in ictihat])\n",
    "    ictihats_punctuation[index] = ' '.join([turkStem.stemWord(word) for word in ictihat.split()])\n",
    "    \n",
    "#print(ictihats_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945fcd6-9d25-4146-8d98-cb76b1c130bd",
   "metadata": {},
   "source": [
    "## write data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54c0132-3234-404c-a793-21b194fb1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['ictihats', 'sucs'])\n",
    "df['ictihats'] = ictihats_punctuation\n",
    "df['sucs'] = suc_array\n",
    "csv_file_name = 'train_set_' + str(word_size) + '.csv'\n",
    "\n",
    "df.to_csv(csv_file_name, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511863a-1224-49c8-ad2b-a47cad91a28d",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a780d958-a973-486e-8e1f-c32564ee22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_csv = pd.read_csv(filename)\n",
    "# column = csv_file_name[]\n",
    "# csv_file_name = 'train_set_' + str(word_size) + '.csv'\n",
    "# my_csv = pd.read_csv(csv_file_name)\n",
    "# #print(my_csv.columns)\n",
    "# x = my_csv['ictihats']\n",
    "# print(x.tolist())\n",
    "# y = my_csv['sucs']\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state=36)\n",
    "#print(x_train)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ictihats_punctuation, \n",
    "                                                    suc_array, \n",
    "                                                    test_size=0.20,random_state=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb6583-90bb-4d17-8ad7-54fbd86347a6",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b320805f-a5fb-4ff1-8d23-5fb0cb168dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onkol\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lahî'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= stopwords)\n",
    "training_data = tfidfvectorizer.fit_transform(x_train)\n",
    "testing_data = tfidfvectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbeb31-1cb7-4469-907e-e91da1c6cfba",
   "metadata": {},
   "source": [
    "## Support Vector Machines (specifically linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a69dc08-6bf8-49ed-998f-702fb4805b35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  78    0    0    0    0    0    0    0    0    0]\n",
      " [   0   14    0    1    0    0    0   71    0    0]\n",
      " [   0    0  133    0    1    0    0    0    0    0]\n",
      " [   0    0    4  284    0    0    0    1    0    1]\n",
      " [   0    0    0    0   62    0    0    0    4    0]\n",
      " [   0    2    0    0    0   53    0    0    0    0]\n",
      " [   0    0    0    0    0    0   62    0    0    0]\n",
      " [   0    3    3    2    0    0    0  103    0    0]\n",
      " [   0    0    0    0    0    0    0    0  133    1]\n",
      " [   0    0    2    1    0    1    2    1    0 2593]]\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "              5607 sayılı Kanuna muhalefet       1.00      1.00      1.00        78\n",
      "                                   Hakaret       0.74      0.16      0.27        86\n",
      "                                 Hırsızlık       0.94      0.99      0.96       134\n",
      "                           Kasten yaralama       0.99      0.98      0.98       290\n",
      "Kullanmak için uyuşturucu madde bulundurma       0.98      0.94      0.96        66\n",
      "           Silahlı terör örgütüne üye olma       0.98      0.96      0.97        55\n",
      "                         Taksirle yaralama       0.97      1.00      0.98        62\n",
      "                                    Tehdit       0.59      0.93      0.72       111\n",
      "           Uyuşturucu madde ticareti yapma       0.97      0.99      0.98       134\n",
      "                                     other       1.00      1.00      1.00      2600\n",
      "\n",
      "                                  accuracy                           0.97      3616\n",
      "                                 macro avg       0.91      0.90      0.88      3616\n",
      "                              weighted avg       0.97      0.97      0.97      3616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(training_data, y_train)\n",
    "y_pred = svclassifier.predict(testing_data)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8458b-83e9-4373-8b27-cb610f2106d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c756d9-52e6-430d-be96-e8324245522a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  75    0    0    0    0    0    0    0    0    3]\n",
      " [   0    0    0    0    0    0    0   64    0   22]\n",
      " [   0    0   82    6    0    0    0    0    0   46]\n",
      " [   0    0    1  256    0    0    0    0    0   33]\n",
      " [   0    0    0    0   12    0    0    0    6   48]\n",
      " [   0    0    0    0    0   50    0    0    0    5]\n",
      " [   0    0    0    2    0    0   52    0    0    8]\n",
      " [   0    0    1    4    0    0    0   90    0   16]\n",
      " [   0    0    0    1    0    0    0    0   37   96]\n",
      " [   2    0    0    1    0    1    0    1    0 2595]]\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "              5607 sayılı Kanuna muhalefet       0.97      0.96      0.97        78\n",
      "                                   Hakaret       0.00      0.00      0.00        86\n",
      "                                 Hırsızlık       0.98      0.61      0.75       134\n",
      "                           Kasten yaralama       0.95      0.88      0.91       290\n",
      "Kullanmak için uyuşturucu madde bulundurma       1.00      0.18      0.31        66\n",
      "           Silahlı terör örgütüne üye olma       0.98      0.91      0.94        55\n",
      "                         Taksirle yaralama       1.00      0.84      0.91        62\n",
      "                                    Tehdit       0.58      0.81      0.68       111\n",
      "           Uyuşturucu madde ticareti yapma       0.86      0.28      0.42       134\n",
      "                                     other       0.90      1.00      0.95      2600\n",
      "\n",
      "                                  accuracy                           0.90      3616\n",
      "                                 macro avg       0.82      0.65      0.68      3616\n",
      "                              weighted avg       0.88      0.90      0.88      3616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onkol\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\onkol\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\onkol\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "\n",
    "# print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "# print('Precision score: ', format(precision_score(y_test, predictions, average='weighted')))\n",
    "# print('Recall score: ', format(recall_score(y_test, predictions ,average='weighted')))\n",
    "# print('F1 score: ', format(f1_score(y_test, predictions ,average='weighted')))\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707aeff-312d-4036-8c95-f46f880a4c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3637a68f-37f8-41b9-9c6e-3fc371d40ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  77    0    0    0    0    0    0    0    0    1]\n",
      " [   0   12    1    1    0    0    0   71    0    1]\n",
      " [   0    0  130    0    1    0    0    0    0    3]\n",
      " [   0    0    4  284    0    0    0    1    0    1]\n",
      " [   0    0    1    0   56    0    0    0    9    0]\n",
      " [   0    2    0    0    0   50    0    0    2    1]\n",
      " [   0    0    0    0    0    0   62    0    0    0]\n",
      " [   0    3    4    2    0    0    0  102    0    0]\n",
      " [   0    0    0    0    1    0    0    0  131    2]\n",
      " [   0    0    1    4    0    1    1    1    0 2592]]\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "              5607 sayılı Kanuna muhalefet       1.00      0.99      0.99        78\n",
      "                                   Hakaret       0.71      0.14      0.23        86\n",
      "                                 Hırsızlık       0.92      0.97      0.95       134\n",
      "                           Kasten yaralama       0.98      0.98      0.98       290\n",
      "Kullanmak için uyuşturucu madde bulundurma       0.97      0.85      0.90        66\n",
      "           Silahlı terör örgütüne üye olma       0.98      0.91      0.94        55\n",
      "                         Taksirle yaralama       0.98      1.00      0.99        62\n",
      "                                    Tehdit       0.58      0.92      0.71       111\n",
      "           Uyuşturucu madde ticareti yapma       0.92      0.98      0.95       134\n",
      "                                     other       1.00      1.00      1.00      2600\n",
      "\n",
      "                                  accuracy                           0.97      3616\n",
      "                                 macro avg       0.90      0.87      0.86      3616\n",
      "                              weighted avg       0.97      0.97      0.96      3616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_regres_classifier = LogisticRegression(random_state = 36)\n",
    "log_regres_classifier.fit(training_data, y_train)\n",
    "y_pred = log_regres_classifier.predict(testing_data)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "# print('Accuracy score: ', (accuracy_score(y_test, y_pred)))\n",
    "# print('Precision score: ', (precision_score(y_test, y_pred, average='weighted')))\n",
    "# print('Recall score: ', (recall_score(y_test, y_pred ,average='weighted')))\n",
    "# print('F1 score: ', (f1_score(y_test, y_pred ,average='weighted')))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4f5c30-bf9c-4386-b1fc-1554137eb9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(len(suc_array))\n",
    "#print(ictihats_punctuation)\n",
    "# index_list = []\n",
    "# for i in range(0,100):\n",
    "#     index_list.append(str(i))\n",
    "\n",
    "# preprocessed_ictihats = []\n",
    "# for i in ictihats_punctuation:\n",
    "#        preprocessed_ictihats.append(list(filter(lambda x: x != \"\",i.split(\" \"))))\n",
    "\n",
    "# #print(preprocessed_ictihats)\n",
    "# turkStem = TurkishStemmer()\n",
    "# for index, ictihat in enumerate(preprocessed_ictihats,start=0):\n",
    "#     preprocessed_ictihats[index] = [turkStem.stemWord(word) for word in ictihat]\n",
    "    \n",
    "#print(preprocessed_ictihats)\n",
    "\n",
    "#tfidf_wm_array\n",
    "#tfidf_wm_array.shape\n",
    "# frequency_matrix = pd.DataFrame(tfidf_wm_array, columns=tfidfvectorizer.get_feature_names())\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# y = np.array([0,1,2,3,4,5,6,7,8,9,10]) \n",
    "\n",
    "\n",
    "# naive_bayes = MultinomialNB()\n",
    "# naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "# predictions = naive_bayes.predict(testing_data)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "# print('Precision score: ', format(precision_score(y_test, predictions,\n",
    "#                                            average='macro')))\n",
    "# print('Recall score: ', format(recall_score(y_test, predictions\n",
    "#                                            ,average='macro')))\n",
    "# print('F1 score: ', format(f1_score(y_test, predictions\n",
    "#                                            ,average='macro')))\n",
    "\n",
    "# print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# svclassifier = SVC(kernel='linear')\n",
    "# svclassifier.fit(training_data, y_train)\n",
    "# y_pred = svclassifier.predict(testing_data)\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred))\n",
    "\n",
    "\n",
    "\n",
    "# frequency_matrix\n",
    "# print(tfidfvectorizer.vocabulary_)\n",
    "# print(tfidf_wm.toarray())\n",
    "# tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "# df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = index_list,columns = tfidf_tokens)\n",
    "# print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "# print(df_tfidfvect)\n",
    "# vectorizer.get_feature_names_out()\n",
    "# ictihat_freq = []\n",
    "\n",
    "\n",
    "# for i in preprocessed_ictihats:\n",
    "#     count_freq = Counter(i)\n",
    "#     ictihat_freq.append(count_freq)\n",
    "    \n",
    "# pprint.pprint(ictihat_freq)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
